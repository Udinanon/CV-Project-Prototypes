{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hand Segmentation CNNs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1vbGTYq_IAIoc9XbvEWRpGXYVbZ1vDu-D",
      "authorship_tag": "ABX9TyM/MLikUSizcQYnk6m77SyT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "30af58aa80544edf85092b7c80f93030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec1eddda68b14120996d95d056ce85fa",
              "IPY_MODEL_4cf7fb52ee1b447dbb818eb7b9d149e9",
              "IPY_MODEL_0511905eb9794eefa1e51bdd8fc601a5"
            ],
            "layout": "IPY_MODEL_ac637207f43441b79c2813c0c9b6af9f"
          }
        },
        "ec1eddda68b14120996d95d056ce85fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dbe1f06f339441da834ee31b2466bce",
            "placeholder": "​",
            "style": "IPY_MODEL_975518e102684505a8f7660533815d92",
            "value": "100%"
          }
        },
        "4cf7fb52ee1b447dbb818eb7b9d149e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b08f27f8d9148929453e1f5e562dc7e",
            "max": 167502836,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95b28eac83b94ad8aaf2c883ee01bcb3",
            "value": 167502836
          }
        },
        "0511905eb9794eefa1e51bdd8fc601a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36e3da16044747999bfcf06355c193f0",
            "placeholder": "​",
            "style": "IPY_MODEL_58714e9326f248648eeb2027db69208d",
            "value": " 160M/160M [00:00&lt;00:00, 231MB/s]"
          }
        },
        "ac637207f43441b79c2813c0c9b6af9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dbe1f06f339441da834ee31b2466bce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "975518e102684505a8f7660533815d92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b08f27f8d9148929453e1f5e562dc7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95b28eac83b94ad8aaf2c883ee01bcb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36e3da16044747999bfcf06355c193f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58714e9326f248648eeb2027db69208d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Udinanon/CV-Project-Prototypes/blob/main/Hand_Segmentation_CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Needed setup  "
      ],
      "metadata": {
        "id": "FnJaKyNXH19c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%ls\n",
        "%cd drive/MyDrive/\n",
        "%cd Colab_Notebooks/faces_datasets/\n",
        "# move to correct folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gJ-CU7vFZzo",
        "outputId": "501cb8e6-ea2e-49dc-ec03-fc409812fca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/Colab_Notebooks/faces_datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove useless files\n",
        "!rm -dr detect_hands/\n",
        "!rm -f egohands_data.zip\n",
        "!rm -dr egohands/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1d7NeD4G-05",
        "outputId": "199ec153-6f30-4646-c851-e937a73a0e86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'egohands/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7q21mKHm2-g"
      },
      "outputs": [],
      "source": [
        "#unpack the hand_pver_face dataset \n",
        "!tar -xvzf hand_over_face_corrected.tar.gz\n",
        "#remove dataset file\n",
        "!rm -f hand_over_face_corrected.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here we start with the actual code\n"
      ],
      "metadata": {
        "id": "uDyJysVwiP7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# basic imports\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "os.getcwd()\n"
      ],
      "metadata": {
        "id": "-qnCfFQxqDI2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d9006b3b-32b6-4887-e57f-abdb2d0eb8c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import Mask RCNN model \n",
        "# https://pytorch.org/vision/master/models/mask_rcnn.html\n",
        "\n",
        "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "Hbj1s6jSDPsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### loading one image and visualizing the model\n",
        "very basic"
      ],
      "metadata": {
        "id": "OSN5FKhCiXc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "imsize = 256\n",
        "\n",
        "loader = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "def image_loader(image_name):\n",
        "    image = Image.open(image_name)\n",
        "    image = loader(image).float()\n",
        "    image = torch.tensor(image, requires_grad=True)\n",
        "    image = image.unsqueeze(0)\n",
        "    return image\n",
        "\n",
        "model(image_loader(\"images/train/CARDS_LIVINGROOM_T_B_frame_0001.jpg\"))"
      ],
      "metadata": {
        "id": "J4l3kIbXKIPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COCO_INSTANCE_CATEGORY_NAMES = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
        "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
        "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
        "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
        "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
        "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]\n",
        "\n",
        "import cv2, random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_path = \"images/train/CARDS_LIVINGROOM_T_B_frame_0001.jpg\"\n",
        "\n",
        "def get_prediction(img_path, threshold):\n",
        "  img = Image.open(img_path)\n",
        "  transform = transforms.Compose([transforms.ToTensor()])\n",
        "  img = transform(img)\n",
        "  pred = model([img])\n",
        "  pred_score = list(pred[0]['scores'].detach().numpy())\n",
        "  pred_t = [pred_score.index(x) for x in pred_score if x>threshold][-1]\n",
        "  masks = (pred[0]['masks']>0.5).squeeze().detach().cpu().numpy()\n",
        "  pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].numpy())]\n",
        "  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())]\n",
        "  masks = masks[:pred_t+1]\n",
        "  pred_boxes = pred_boxes[:pred_t+1]\n",
        "  pred_class = pred_class[:pred_t+1]\n",
        "  return masks, pred_boxes, pred_class\n",
        "\n",
        "\n",
        "def random_colour_masks(image):\n",
        "  colours = [[0, 255, 0],[0, 0, 255],[255, 0, 0],[0, 255, 255],[255, 255, 0],[255, 0, 255],[80, 70, 180],[250, 80, 190],[245, 145, 50],[70, 150, 250],[50, 190, 190]]\n",
        "  r = np.zeros_like(image).astype(np.uint8)\n",
        "  g = np.zeros_like(image).astype(np.uint8)\n",
        "  b = np.zeros_like(image).astype(np.uint8)\n",
        "  r[image == 1], g[image == 1], b[image == 1] = colours[random.randrange(0,10)]\n",
        "  coloured_mask = np.stack([r, g, b], axis=2)\n",
        "  return coloured_mask\n",
        "\n",
        "def instance_segmentation_api(img_path, threshold=0.5, rect_th=3, text_size=3, text_th=3):\n",
        "  masks, boxes, pred_cls = get_prediction(img_path, threshold)\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  for i in range(len(masks)):\n",
        "    rgb_mask = random_colour_masks(masks[i])\n",
        "    img = cv2.addWeighted(img, 1, rgb_mask, 0.5, 0)\n",
        "    cv2.rectangle(img, boxes[i][0], boxes[i][1],color=(0, 255, 0), thickness=rect_th)\n",
        "    cv2.putText(img,pred_cls[i], boxes[i][0], cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th)\n",
        "  plt.figure(figsize=(20,30))\n",
        "  plt.imshow(img)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.show()\n",
        "\n",
        "instance_segmentation_api(image_path)\n"
      ],
      "metadata": {
        "id": "H4Tylf_eMOaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BTW how to read all sys info on the machine"
      ],
      "metadata": {
        "id": "leMIYzvYidGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "print(\"CPU\")\n",
        "!cat /proc/cpuinfo\n",
        "print(\"RAM\")\n",
        "!cat /proc/meminfo\n",
        "\n",
        "print(\"GPU?\")\n",
        "tf.test.gpu_device_name()\n",
        "device_lib.list_local_devices()"
      ],
      "metadata": {
        "id": "LcvpmyuPOn2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "root = os.getcwd()\n",
        "path = os.path.join(root, \"images\", \"test\")\n",
        "imgs = [x for x in sorted(os.listdir(path)) if x.endswith(\".jpg\")]\n",
        "print(\"list\", len(imgs))\n",
        "print(\"folder\", len(list(os.listdir(path))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n27MFLMLle20",
        "outputId": "f6fdc3d1-9e25-42f2-c39f-7cf79d1c9c84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "list 36\n",
            "folder 37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the datasets\n",
        "import os\n",
        "import numpy as np\n",
        "import csv\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class EgoHandsDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, root, transforms, test = False):\n",
        "    self.root = root\n",
        "    self.transforms = transforms\n",
        "    # load all image files, sorting them to\n",
        "    # ensure that they are aligned\n",
        "    if test:\n",
        "      path = os.path.join(root, \"images\", \"test\")\n",
        "    else:\n",
        "      path = os.path.join(root, \"images\", \"train\")\n",
        "\n",
        "    self.imgs = [x for x in sorted(os.listdir(path)) if x.endswith(\".jpg\")]\n",
        "    mask_file_path = [x for x in sorted(os.listdir(path)) if x.endswith(\".csv\")][0]\n",
        "    print(mask_file_path)\n",
        "    print(os.path.join(path, mask_file_path))\n",
        "    self.mask_entries = []\n",
        "    with open(os.path.join(path, mask_file_path)) as mask_file:\n",
        "      mask_csv = csv.reader(mask_file)\n",
        "      next(mask_csv)\n",
        "      for line in mask_csv:\n",
        "        self.mask_entries.append([line[0], int(line[4]), int(line[5]), int(line[6]), int(line[7])])\n",
        "    print(\"LEN IMGS\", len(self.imgs))\n",
        "    print(\"LEN MASK\", len(self.mask_entries))\n",
        "    \n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # load image\n",
        "    img_path = os.path.join(self.root, \"images\", \"test\", self.imgs[idx])\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    # read the entries for the bounding boxes for this image\n",
        "    image_filename = \"images/test/\" + self.imgs[idx]        \n",
        "    entries = [entry for entry in self.mask_entries if entry[0] == image_filename]\n",
        "\n",
        "    # get bounding box coords\n",
        "    num_objs = len(entries)\n",
        "    boxes = []\n",
        "    for entry in entries:\n",
        "      boxes.append([entry[1], entry[2], entry[3], entry[4]])\n",
        "\n",
        "    # convert everything into a torch.Tensor\n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "    # there is only one class\n",
        "    labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "    image_id = torch.tensor([idx])\n",
        "    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "    # suppose all instances are not crowd\n",
        "    iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"image_id\"] = image_id\n",
        "    target[\"area\"] = area\n",
        "    target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "    if self.transforms is not None:\n",
        "      img, target = self.transforms(img, target)\n",
        "\n",
        "    return img, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)\n",
        "\n",
        "\n",
        "# this code come from https://github.com/pytorch/vision/blob/6315358dd06e3a2bcbe9c1e8cdaa10898ac2b308/references/detection/transforms.py#L17\n",
        "# due to the transforms needing to be able to work both on the oimage and the boxes, for data augmentation\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, image, target):\n",
        "        image = F.to_tensor(image)\n",
        "        return image, target\n",
        "\n",
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "def get_transform():\n",
        "    transforms = []\n",
        "    transforms.append(ToTensor())\n",
        "    return Compose(transforms)\n",
        "\n",
        "print(os.getcwd())\n",
        "\n"
      ],
      "metadata": {
        "id": "WnpPE1RYilAE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb425443-dba2-412c-f630-ec73dbde03b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab_Notebooks/faces_datasets\n",
            "test_labels.csv\n",
            "./images/test/test_labels.csv\n",
            "LEN IMGS 36\n",
            "LEN MASK 108\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[460., 177., 984., 653.],\n",
              "        [430., 274., 488., 444.]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "# from https://github.com/pytorch/vision/blob/main/references/detection/utils.py again\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "egohands = EgoHandsDataset(\".\", transforms = get_transform(), test = True)\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        " egohands, batch_size=2, shuffle=True, num_workers=4,\n",
        " collate_fn=collate_fn)\n",
        "# For Training\n",
        "images,targets = next(iter(data_loader))\n",
        "images = list(image for image in images)\n",
        "targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "output = model(images,targets)   # Returns losses and detections\n",
        "# For inference\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199,
          "referenced_widgets": [
            "30af58aa80544edf85092b7c80f93030",
            "ec1eddda68b14120996d95d056ce85fa",
            "4cf7fb52ee1b447dbb818eb7b9d149e9",
            "0511905eb9794eefa1e51bdd8fc601a5",
            "ac637207f43441b79c2813c0c9b6af9f",
            "4dbe1f06f339441da834ee31b2466bce",
            "975518e102684505a8f7660533815d92",
            "6b08f27f8d9148929453e1f5e562dc7e",
            "95b28eac83b94ad8aaf2c883ee01bcb3",
            "36e3da16044747999bfcf06355c193f0",
            "58714e9326f248648eeb2027db69208d"
          ]
        },
        "id": "-MYtUbpuu8cF",
        "outputId": "529a2a3e-6a4e-4cfc-c384-dc53d406d862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/160M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30af58aa80544edf85092b7c80f93030"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_labels.csv\n",
            "./images/test/test_labels.csv\n",
            "LEN IMGS 36\n",
            "LEN MASK 108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zyNhESCO4HIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## This dataset is fucking trash jesus\n",
        "#  thanks https://github.com/guglielmocamporese/hands-segmentation-pytorch/blob/master/dataloader.py\n",
        "class HandOverFaceDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, root, transforms):\n",
        "    self.root = root\n",
        "    self.transforms = transforms\n",
        "    # load all image files, sorting them to\n",
        "    # ensure that they are aligned\n",
        "    self.imgs = list(sorted(os.listdir(os.path.join(root, \"hand_over_face\", \"images_resized\"))))\n",
        "    self.masks = list(sorted(os.listdir(os.path.join(root, \"hand_over_face\", \"masks\"))))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # load images and masks\n",
        "    img_path = os.path.join(self.root, \"hand_over_face\", \"images_resized\", self.imgs[idx])\n",
        "    mask_path = os.path.join(self.root, \"hand_over_face\", \"masks\", self.masks[idx])\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    # note that we haven't converted the mask to RGB,\n",
        "    # because each color corresponds to a different instance\n",
        "    # with 0 being background\n",
        "\n",
        "    ## YEAH that's not true here, we need to count and separate the masks somehow\n",
        "    ## the annotations have some info, maybe we can use those??\n",
        "    mask = Image.open(mask_path)\n",
        "    # convert the PIL Image into a numpy array\n",
        "    mask = np.array(mask)\n",
        "    # instances are encoded as different colors\n",
        "    obj_ids = np.unique(mask)\n",
        "    # first id is the background, so remove it\n",
        "    obj_ids = obj_ids[1:]\n",
        "\n",
        "    # split the color-encoded mask into a set\n",
        "    # of binary masks\n",
        "    masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "    # get bounding box coordinates for each mask\n",
        "    num_objs = len(obj_ids)\n",
        "    boxes = []\n",
        "    for i in range(num_objs):\n",
        "        pos = np.where(masks[i])\n",
        "        xmin = np.min(pos[1])\n",
        "        xmax = np.max(pos[1])\n",
        "        ymin = np.min(pos[0])\n",
        "        ymax = np.max(pos[0])\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "    # convert everything into a torch.Tensor\n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "    # there is only one class\n",
        "    labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "    masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "    image_id = torch.tensor([idx])\n",
        "    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "    # suppose all instances are not crowd\n",
        "    iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"masks\"] = masks\n",
        "    target[\"image_id\"] = image_id\n",
        "    target[\"area\"] = area\n",
        "    target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "    if self.transforms is not None:\n",
        "        img, target = self.transforms(img, target)\n",
        "\n",
        "    return img, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)"
      ],
      "metadata": {
        "id": "E8mEzjDcmx07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# HandOverFace (HOF)\n",
        "mkdir -p \"./HOF\"\n",
        "gdown \"https://drive.google.com/uc?id=1hHUvINGICvOGcaDgA5zMbzAIUv7ewDd3\" -O \"./HOF/hand_over_face_corrected.tar.gz\"\n",
        "tar -xvf \"./HOF/hand_over_face_corrected.tar.gz\" -C \"./HOF\"\n",
        "rm \"./HOF/hand_over_face_corrected.tar.gz\""
      ],
      "metadata": {
        "id": "wMt5Z1st5uES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Download datasets\n",
        "DATA_BASE_PATH=\"./EGO\"\n",
        "mkdir -p \"${DATA_BASE_PATH}\"\n",
        "\n",
        "# EgoHands\n",
        "wget \"http://vision.soic.indiana.edu/egohands_files/egohands_data.zip\" -O \"${DATA_BASE_PATH}/egohands_data.zip\"\n",
        "unzip \"${DATA_BASE_PATH}/egohands_data.zip\" -d \"${DATA_BASE_PATH}/egohands_data\"\n",
        "rm \"${DATA_BASE_PATH}/egohands_data.zip\""
      ],
      "metadata": {
        "id": "VgxGkjlO6Qcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "from pprint import pprint\n",
        "from PIL import Image, ImageDraw\n",
        "## allows us to geenrate masks from the egohands datasets and not just boxes\n",
        "# all from https://github.com/guglielmocamporese/hands-segmentation-pytorch/blob/d4643f3a2137e90e60b8eba0d5bb06bb25552841/dataloader.py#L356\n",
        "\n",
        "data_base_path = os.path.join(\"EGO\", \"egohands_data\")\n",
        "frame_tmpl='frame_{:04d}.jpg'\n",
        "metadata = scipy.io.loadmat(os.path.join(data_base_path, 'metadata.mat'))\n",
        "annotations = metadata['video'][0]\n",
        "x = list(annotations[0])\n",
        "video_id, _, _, _, _, _, labeled_frames = x # more info the readme\n",
        "video_id = video_id[0]\n",
        "labeled_frames = labeled_frames[0]\n",
        "frame_ann = labeled_frames[0]\n",
        "frame_id = frame_ann[0].reshape(-1)[0]\n",
        "image_paths = []\n",
        "masks_poly = []\n",
        "polygons = []\n",
        "for idx, ll in enumerate(frame_ann):\n",
        "  if (idx > 0) and len(ll) > 0:\n",
        "    p = [tuple(pp) for pp in ll]\n",
        "    polygons += [p]\n",
        "print(len(polygons))\n",
        "image_path = os.path.join(data_base_path, '_LABELLED_SAMPLES', video_id, frame_tmpl.format(frame_id))\n",
        "image = Image.open(image_path)\n",
        "image\n",
        "\n",
        "\n",
        "w, h = image.size\n",
        "mask = Image.new('L', (w, h), 0)\n",
        "ImageDraw.Draw(mask).polygon(polygons[1], outline=255, fill=255)\n",
        "mask\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "NDPmE7d18UXd",
        "outputId": "26ebfc65-e2c7-4793-f2e4-3d901bdb8020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=1280x720 at 0x7F6A767CC390>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EZAO2cnO8mX8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}