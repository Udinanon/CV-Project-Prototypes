{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hand Segmentation CNNs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1vbGTYq_IAIoc9XbvEWRpGXYVbZ1vDu-D",
      "authorship_tag": "ABX9TyPEQyMoqGRqH5rRRZ/s2XzV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Needed setup  "
      ],
      "metadata": {
        "id": "FnJaKyNXH19c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "%ls\n",
        "%cd drive/MyDrive/\n",
        "%cd Colab_Notebooks/faces_datasets/\n",
        "# move to correct folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gJ-CU7vFZzo",
        "outputId": "b1abadbd-99f4-4b2a-b82d-4e1529c7764a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "egohands_dataset_to_csv.py  \u001b[0m\u001b[01;34mhand_over_face\u001b[0m/  \u001b[01;34mimages\u001b[0m/\n",
            "[Errno 2] No such file or directory: 'drive/MyDrive/'\n",
            "/content/drive/MyDrive/Colab Notebooks/faces_datasets\n",
            "[Errno 2] No such file or directory: 'Colab_Notebooks/faces_datasets/'\n",
            "/content/drive/MyDrive/Colab Notebooks/faces_datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# copied from https://towardsdatascience.com/train-an-object-detector-using-tensorflow-2-object-detection-api-in-2021-a4fed450d1b9\n",
        "# use script to download and \"setup\" egohands\n",
        "!git clone https://github.com/aalpatya/detect_hands.git\n",
        "!cp detect_hands/egohands_dataset_to_csv.py .\n",
        "!python egohands_dataset_to_csv.py"
      ],
      "metadata": {
        "id": "nVf6DK3iGDUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove useless files\n",
        "!rm -dr detect_hands/\n",
        "!rm -f egohands_data.zip\n",
        "!rm -dr egohands/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1d7NeD4G-05",
        "outputId": "199ec153-6f30-4646-c851-e937a73a0e86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'egohands/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7q21mKHm2-g"
      },
      "outputs": [],
      "source": [
        "#unpack the hand_pver_face dataset \n",
        "!tar -xvzf hand_over_face_corrected.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove dataset file\n",
        "!rm -f hand_over_face_corrected.tar.gz\n"
      ],
      "metadata": {
        "id": "z6qu0xpPHuzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here we start with the actual code\n"
      ],
      "metadata": {
        "id": "uDyJysVwiP7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# basic imports\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "os.getcwd()\n"
      ],
      "metadata": {
        "id": "-qnCfFQxqDI2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d9006b3b-32b6-4887-e57f-abdb2d0eb8c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import Mask RCNN model \n",
        "# https://pytorch.org/vision/master/models/mask_rcnn.html\n",
        "\n",
        "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "Hbj1s6jSDPsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### loading one image and visualizing the model\n",
        "very basic"
      ],
      "metadata": {
        "id": "OSN5FKhCiXc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "imsize = 256\n",
        "\n",
        "loader = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "def image_loader(image_name):\n",
        "    image = Image.open(image_name)\n",
        "    image = loader(image).float()\n",
        "    image = torch.tensor(image, requires_grad=True)\n",
        "    image = image.unsqueeze(0)\n",
        "    return image\n",
        "\n",
        "model(image_loader(\"images/train/CARDS_LIVINGROOM_T_B_frame_0001.jpg\"))"
      ],
      "metadata": {
        "id": "J4l3kIbXKIPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COCO_INSTANCE_CATEGORY_NAMES = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
        "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
        "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
        "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
        "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
        "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]\n",
        "\n",
        "import cv2, random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_path = \"images/train/CARDS_LIVINGROOM_T_B_frame_0001.jpg\"\n",
        "\n",
        "def get_prediction(img_path, threshold):\n",
        "  img = Image.open(img_path)\n",
        "  transform = transforms.Compose([transforms.ToTensor()])\n",
        "  img = transform(img)\n",
        "  pred = model([img])\n",
        "  pred_score = list(pred[0]['scores'].detach().numpy())\n",
        "  pred_t = [pred_score.index(x) for x in pred_score if x>threshold][-1]\n",
        "  masks = (pred[0]['masks']>0.5).squeeze().detach().cpu().numpy()\n",
        "  pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].numpy())]\n",
        "  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())]\n",
        "  masks = masks[:pred_t+1]\n",
        "  pred_boxes = pred_boxes[:pred_t+1]\n",
        "  pred_class = pred_class[:pred_t+1]\n",
        "  return masks, pred_boxes, pred_class\n",
        "\n",
        "\n",
        "def random_colour_masks(image):\n",
        "  colours = [[0, 255, 0],[0, 0, 255],[255, 0, 0],[0, 255, 255],[255, 255, 0],[255, 0, 255],[80, 70, 180],[250, 80, 190],[245, 145, 50],[70, 150, 250],[50, 190, 190]]\n",
        "  r = np.zeros_like(image).astype(np.uint8)\n",
        "  g = np.zeros_like(image).astype(np.uint8)\n",
        "  b = np.zeros_like(image).astype(np.uint8)\n",
        "  r[image == 1], g[image == 1], b[image == 1] = colours[random.randrange(0,10)]\n",
        "  coloured_mask = np.stack([r, g, b], axis=2)\n",
        "  return coloured_mask\n",
        "\n",
        "def instance_segmentation_api(img_path, threshold=0.5, rect_th=3, text_size=3, text_th=3):\n",
        "  masks, boxes, pred_cls = get_prediction(img_path, threshold)\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  for i in range(len(masks)):\n",
        "    rgb_mask = random_colour_masks(masks[i])\n",
        "    img = cv2.addWeighted(img, 1, rgb_mask, 0.5, 0)\n",
        "    cv2.rectangle(img, boxes[i][0], boxes[i][1],color=(0, 255, 0), thickness=rect_th)\n",
        "    cv2.putText(img,pred_cls[i], boxes[i][0], cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th)\n",
        "  plt.figure(figsize=(20,30))\n",
        "  plt.imshow(img)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.show()\n",
        "\n",
        "instance_segmentation_api(image_path)\n"
      ],
      "metadata": {
        "id": "H4Tylf_eMOaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BTW how to read all sys info on the machine"
      ],
      "metadata": {
        "id": "leMIYzvYidGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "print(\"CPU\")\n",
        "!cat /proc/cpuinfo\n",
        "print(\"RAM\")\n",
        "!cat /proc/meminfo\n",
        "\n",
        "print(\"GPU?\")\n",
        "tf.test.gpu_device_name()\n",
        "device_lib.list_local_devices()"
      ],
      "metadata": {
        "id": "LcvpmyuPOn2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "root = os.getcwd()\n",
        "path = os.path.join(root, \"images\", \"test\")\n",
        "imgs = [x for x in sorted(os.listdir(path)) if x.endswith(\".jpg\")]\n",
        "print(\"list\", len(imgs))\n",
        "print(\"folder\", len(list(os.listdir(path))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n27MFLMLle20",
        "outputId": "f6fdc3d1-9e25-42f2-c39f-7cf79d1c9c84"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "list 36\n",
            "folder 37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import csv\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class EgoHandsDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, root, transforms, test = False):\n",
        "    self.root = root\n",
        "    self.transforms = transforms\n",
        "    # load all image files, sorting them to\n",
        "    # ensure that they are aligned\n",
        "    if test:\n",
        "      path = os.path.join(root, \"images\", \"test\")\n",
        "    else:\n",
        "      path = os.path.join(root, \"images\", \"train\")\n",
        "\n",
        "    self.imgs = [x for x in sorted(os.listdir(path)) if x.endswith(\".jpg\")]\n",
        "    mask_file_path = [x for x in sorted(os.listdir(path)) if x.endswith(\".csv\")][0]\n",
        "    self.mask_entries = []\n",
        "    with open(self.mask_file_path) as mask_file:\n",
        "      mask_csv = csv.reader(mask_file)\n",
        "      for line in mask_csv:\n",
        "        self.mask_entries.append([line[0], line[4], line[5], line[6], line[7]])\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # load image\n",
        "    img_path = os.path.join(self.root, \"images\", \"test\", self.imgs[idx])\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    # read the entries for the bounding boxes for this image\n",
        "    image_filename = \"images/test/\" + self.imgs[idx]        \n",
        "    entries = [entry for entry in self.mask_entries if entry[0] == image_filename]\n",
        "\n",
        "    # get bounding box coords\n",
        "    num_objs = len(entries)\n",
        "    boxes = []\n",
        "    for entry in entries:\n",
        "      boxes.append([entry[1], entry[2], entry[3], entry[4]])\n",
        "\n",
        "    # convert everything into a torch.Tensor\n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "    # there is only one class\n",
        "    labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "    image_id = torch.tensor([idx])\n",
        "    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "    # suppose all instances are not crowd\n",
        "    iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"image_id\"] = image_id\n",
        "    target[\"area\"] = area\n",
        "    target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "    if self.transforms is not None:\n",
        "      img, target = self.transforms(img, target)\n",
        "\n",
        "    return img, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)\n",
        "\n",
        "class HandOverFaceDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, root, transforms):\n",
        "    self.root = root\n",
        "    self.transforms = transforms\n",
        "    # load all image files, sorting them to\n",
        "    # ensure that they are aligned\n",
        "    self.imgs = list(sorted(os.listdir(os.path.join(root, \"hand_over_face\", \"images_resized\"))))\n",
        "    self.masks = list(sorted(os.listdir(os.path.join(root, \"hand_over_face\", \"masks\"))))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # load images and masks\n",
        "    img_path = os.path.join(self.root, \"hand_over_face\", \"images_resized\", self.imgs[idx])\n",
        "    mask_path = os.path.join(self.root, \"hand_over_face\", \"masks\", self.masks[idx])\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    # note that we haven't converted the mask to RGB,\n",
        "    # because each color corresponds to a different instance\n",
        "    # with 0 being background\n",
        "\n",
        "    ## YEAH that's not true here, we need to count and separate the masks somehow\n",
        "    ## the annotations have some info, maybe we can use those??\n",
        "    mask = Image.open(mask_path)\n",
        "    # convert the PIL Image into a numpy array\n",
        "    mask = np.array(mask)\n",
        "    # instances are encoded as different colors\n",
        "    obj_ids = np.unique(mask)\n",
        "    # first id is the background, so remove it\n",
        "    obj_ids = obj_ids[1:]\n",
        "\n",
        "    # split the color-encoded mask into a set\n",
        "    # of binary masks\n",
        "    masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "    # get bounding box coordinates for each mask\n",
        "    num_objs = len(obj_ids)\n",
        "    boxes = []\n",
        "    for i in range(num_objs):\n",
        "        pos = np.where(masks[i])\n",
        "        xmin = np.min(pos[1])\n",
        "        xmax = np.max(pos[1])\n",
        "        ymin = np.min(pos[0])\n",
        "        ymax = np.max(pos[0])\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "    # convert everything into a torch.Tensor\n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "    # there is only one class\n",
        "    labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "    masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "    image_id = torch.tensor([idx])\n",
        "    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "    # suppose all instances are not crowd\n",
        "    iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"masks\"] = masks\n",
        "    target[\"image_id\"] = image_id\n",
        "    target[\"area\"] = area\n",
        "    target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "    if self.transforms is not None:\n",
        "        img, target = self.transforms(img, target)\n",
        "\n",
        "    return img, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)"
      ],
      "metadata": {
        "id": "WnpPE1RYilAE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}